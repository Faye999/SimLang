{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Language 19, Greenbergian Universals (lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simulation features a replication of\n",
    "the Culbertson & Smolensky model of learning biases for Greenberg’s Universal 18.\n",
    "\n",
    "|.        | N-Adj | Adj-N |\n",
    "|---------|-------|-------|\n",
    "|**Num-N**|  17%  | 27%   |\n",
    "|**N-Num**|  52%  | 4%    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the usual stuff to import modules and set up plotting. Note that there are some extra functions here to handle various ways of generating random numbers and deal with probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')\n",
    "\n",
    "from scipy.stats import beta, binom\n",
    "from math import log, exp\n",
    "from scipy.misc import logsumexp\n",
    "from numpy.random import choice, binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The model uses Bayesian inference to predict the types of grammars learners will infer given (1) a set of counts of [Adj-N, N-Adj, Num-N, N-Num] utterances and (2) their prior expectations in terms of variation and ordering combinations. The first part of code imports what we’ll need to use the binomial and beta distributions, to do logs and exponentials, and to generate random numbers of various kinds.\n",
    "\n",
    "## The input data\n",
    "\n",
    "The input consists of counts of [Adj-N, N-Adj, Num-N, N-Num] with 40 total per modifier type. Whether the input is skewed toward pre- or post-nominal modifiers for each phrase type depends on the condition, as shown in the table.\n",
    "\n",
    "\n",
    "|condition   | training counts|\n",
    "|------------|---------------|\n",
    "|1 | [28,12,28,12] |\n",
    "|2 | [12,28,12,28] |\n",
    "|3 | [12,28,28,12] |\n",
    "|4 | [28,12,12,28] |\n",
    "\n",
    "As in your previous lab, we’ll use a grid of probabilities to describe the space of possible generating grammars. This time, we’ll need to use the grid twice–once for the probability of Adj-N (vs N-Adj), and again for the probability of Num-N (vs. N-Num)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[28, 12, 28, 12], [12, 28, 12, 28], [12, 28, 28, 12], [28, 12, 12, 28]]\n",
    "\n",
    "grid_granularity = 100 \n",
    "possible_p = []\n",
    "for i in range(1, grid_granularity):\n",
    "    possible_p.append(i / (grid_granularity + 0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check out `training_data`. How would you access just the counts for condition 2? Also take a look at the grid. How is it different from the one you used in the previous lab?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood\n",
    "\n",
    "The likelihood here is calculated using the binomial distribution, the same you used to calculated the likelihood of a particular sequence of word 0’s and word 1’s in the previous lab. Only difference is that here we don’t care what order they were heard in, just about the number of Adj-N out of all Adj trials, and the number of Num-N out of all Num trials.\n",
    "\n",
    "The function below calculates log likelihood of data, (where data are counts representing number of Adj-N out of total Adj instances and number of Num-N out of total Num instances) given point probability of Adj-N, and Num-N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_likelihood(data, p_AdjN, p_NumN):\n",
    "    loglikelihood = []\n",
    "    loglikelihood_AdjN = binom.logpmf(data[0], data[0] + data[1], p_AdjN)\n",
    "    loglikelihood_NumN = binom.logpmf(data[2], data[2] + data[3], p_NumN)\n",
    "    loglikelihood = loglikelihood_AdjN + loglikelihood_NumN #summing logs = multiplying non-log\n",
    "    return loglikelihood   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check out U18_likelihood. Notice that it returns log probabilities. Find the line in the function that would allow you to calculate the likelihood of getting 28 Adj-N counts out of a total of 40 when the underlying probability of Adj-N according to the grammar is 0.7 compared to 0.3. Calculate it!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior\n",
    "\n",
    "The prior in this model has two parts. One part you know about already: regularization as encoded by the parameters of the beta distribution. In the last lab you used a single parameter, alpha, for a single symmetrical beta distribution. Here we want two separate asymmetrical beta distributions–favoring either probabilities close to one or close to zero, but not both.\n",
    "\n",
    "The two parameters of the beta distribution are (annoyingly) called alpha and beta. To get the second part of the prior–the part which favors particular combinations of orders (i.e., Adj-N with Num-N) we need to promote or penalize particular parts of the two dimensional grammar space. \n",
    "\n",
    "![alt text](http://ling.ed.ac.uk/~simon/simlang/fig2.jpg \"grammar space\")\n",
    "\n",
    "We can do this by defining four “components\" using different combinations of alpha and beta, with alpha constrained to be higher than beta. The graph below shows beta distributions with (alpha=10, beta=10), (alpha=15, beta=3), (alpah=15, beta=0.1) for Adj-N, and the reverse for Num-N:\n",
    "\n",
    "![alt text](http://ling.ed.ac.uk/~simon/simlang/fig3.jpg \"beta distributions\")\n",
    "\n",
    "Given these four components, we calculate the prior probability of any grammar. This will actually be the sum of its probability given the beta distributions governing p(Adj-N) and p(Num-N) for each component, weighted by the probability of that component. The latter is determined by g. If the probability of a given component (e.g., g[4]) is low, then even grammars very likely to be generated by it will have a low prior probability.\n",
    "\n",
    "The function below calculates the log prior probability of a given p_AdjN and p_NumN given a set of parameters. This is a sum over the probabilities given by each mixture component. Parameters are: g (set of four mixture weights), a(lpha), b(eta) (Beta shape parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_prior(g, a, b, p_AdjN, p_NumN):\n",
    "    pattern1_component = [a, b, a, b] # higher prob for Adj-N, Num-N\n",
    "    pattern2_component = [b, a, b, a] # higher prob for N-Adj, N-Num\n",
    "    pattern3_component = [b, a, a, b] # higher prob for N-Adj, Num-N\n",
    "    pattern4_component = [a, b, b, a] # higher prob for Adj-N, N-Num\n",
    "    components = [pattern1_component, pattern2_component, \n",
    "                  pattern3_component, pattern4_component]\n",
    "    logprior = []\n",
    "    for i in range(0,4): # loop over all four components\n",
    "        logprior_i_Adj = beta.logpdf(p_AdjN, components[i][0], components[i][1])\n",
    "        logprior_i_Num = beta.logpdf(p_NumN, components[i][2], components[i][3])\n",
    "        logprior_i = logprior_i_Adj + logprior_i_Num\n",
    "        logprior.append(logprior_i)\n",
    "    # a+b+... in log space = log(exp(a)+exp(b)+...)\n",
    "    logprior = log((g[0] * exp(logprior[0])) + \n",
    "                   (g[1] * exp(logprior[1])) + \n",
    "                   (g[2] * exp(logprior[2])) + \n",
    "                   (g[3] * exp(logprior[3])))\n",
    "    return logprior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take a look at the four components in the prior. Can you see why they are defined by those combinations of alpha and beta?*\n",
    "\n",
    "*Each component - a combination of two beta distributions - can in principle generate any grammar, that is any pair p(Adj-N), p(Num-N). Which component assigns the highest probability to the pair (0.8,0.8)? How about (0.4,0.9)?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimal prior parameters\n",
    "\n",
    "The parameters of the prior are alpha, beta, g. The first two (called a, and b in the code) encode the regularization bias. The higher a is relative to b, the more regularization. The third, g is a set of weights for the four components, summing to 1. We searched through these parameters to find the ones that fit our experimental data (shown in the plot below) best.\n",
    "\n",
    "![alt text](http://ling.ed.ac.uk/~simon/simlang/fig4.jpg \"experimental data\")\n",
    "\n",
    "The best fit parameters were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                  g                           alpha, beta\n",
    "fit_parameters = [[0.6293, 0.3706, 0.0001, 0], 16.5,  0.001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior\n",
    "\n",
    "The posterior distribution over grammars can now be calculated using the likelihood and the prior with the fit parameters. This function calculates the log posterior probability of a set of counts for all possible p_AdjN, p_NumN combinations, given prior parameters g, a(lpha), b(eta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_posterior(g, a, b, data):\n",
    "    posterior = [] \n",
    "    for p_a in range(len(possible_p)):\n",
    "        for p_n in range(len(possible_p)):\n",
    "            lik_i = U18_likelihood(data, possible_p[p_a], possible_p[p_n])\n",
    "            prior_i = U18_prior(g, a, b, possible_p[p_a], possible_p[p_n]) \n",
    "            posterior.append(lik_i + prior_i)  \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Given the fit parameters, will the model predict a weak or strong regularization bias? Will a set of counts generated from a grammar like (0.8,0.2) have a high, medium, low posterior probability?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "\n",
    "The model can now be used to generate predicted learning outcomes; given some input data, and the prior parameters, we can generate sampled grammars and plot them in the two-dimensional grammar space we’ve be using. The figure belows shows this as reported in the original Culbertson & Smolensky paper. \n",
    "\n",
    "![alt text](http://ling.ed.ac.uk/~simon/simlang/fig5.jpg \"culbertson & smolensky results\")\n",
    "\n",
    "You can use the `U18_roulette_wheel` function to reproduce this. This function is similar to those you’ve previously used to sample probabilities. The difference is that this function samples pairs of p(Adj-N), p(Num-N) probabilities, and it lets you specify how many such samples you want. You’ll notice there’s also a normalization function `normalize_log_distribution` which takes log probabilities output by the posterior and makes them a proper (non-log) probability distribution for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_log_distribution(distribution):\n",
    "    exp_dist = []\n",
    "    for logp in distribution:\n",
    "        exp_dist.append(exp(logp))\n",
    "    norm_dist = []\n",
    "    for p in exp_dist:\n",
    "        norm_dist.append(p / sum(exp_dist))\n",
    "    return norm_dist\n",
    "\n",
    "\n",
    "def U18_roulette_wheel(g, a, b, data, num_samps):\n",
    "    post = U18_posterior(g, a, b, data) # calculate posterior given training data and prior parameters\n",
    "    post = normalize_log_distribution(post)\n",
    "    # make a grid of all possible p_AdjN, p_NumN combinations at the granularity specified\n",
    "    grid = []\n",
    "    for p_a in range(len(possible_p)):\n",
    "        for p_n in range(len(possible_p)):\n",
    "            grid.append([possible_p[p_a], possible_p[p_n]])\n",
    "    # samples some grammars!\n",
    "    grammars = []\n",
    "    for i in range(num_samps):\n",
    "        r = choice(a=range(len(grid)), p=post)   # choose an index from the grid according to it's posterior probability  \n",
    "        grammars.append(grid[r])\n",
    "    return grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function to plot the results as a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grammars(g_1, g_2, g_3, g_4):\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for g in g_1:\n",
    "        plt.scatter([g[0]], [g[1]], color='red', alpha=0.5)\n",
    "\n",
    "    for g in g_2:\n",
    "        plt.scatter([g[0]], [g[1]], color='blue', alpha=0.5)\n",
    "\n",
    "    for g in g_3:\n",
    "        plt.scatter([g[0]], [g[1]], color='orange', alpha=0.5)\n",
    "\n",
    "    for g in g_4:\n",
    "        plt.scatter([g[0]], [g[1]], color='purple', alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"P(Adj-N)\")\n",
    "    plt.ylabel(\"P(Num-N)\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Generate samples `g_1, g_2, g_3, g_4` using function calls like:\n",
    "```python\n",
    "g_1 = U18_roulette_wheel(fit_parameters[0], fit_parameters[1], fit_parameters[2], training_data[0], 100)\n",
    "```\n",
    "Then call the plotting function to plot them. Does your plot look like figure above from the paper? (N.B. This might take a while to run.)\n",
    "\n",
    "2. What does it mean that the highest value of g is for component 1? Why do you think that might be the highest given the population of learners tested in the experiment? Come up with a new set of g values that you think would more accurately reflect the typology (e.g., in table at the start of the notebook) and redo the samples and plot. Did it turn out as you expected?\n",
    "\n",
    "3. What do you think would happen if the regularization bias were not as strong? Change the a,b parameters and see if you were right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking it further: iterating the model (optional)\n",
    "\n",
    "The remaining functions in this notebook are for iterating the model - taking the original training data, generating posterior probability distributions for all four conditions, sampling some grammars from each posterior and counting the number of each pattern type that results. Then, generating some new training data from one of the sampled grammars from each condition to pass on to the next generation. And so on.\n",
    "\n",
    "The first function classifies a given grammar as one of the four patterns. So if a learner acquires (0.7,0.8) that would be classified as a pattern 1 grammar. We’ll need this to track the counts of each pattern type over generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_classify(p_AdjN, p_NumN):\n",
    "    if p_AdjN > 0.5 and p_NumN > 0.5: return 1\n",
    "    if p_AdjN < 0.5 and p_NumN < 0.5: return 2 \n",
    "    if p_AdjN < 0.5 and p_NumN > 0.5: return 3\n",
    "    if p_AdjN > 0.5 and p_NumN < 0.5: return 4\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function generates training data given a grammar by taking sampled counts from a binomial distribution. We’ll need this to create new training data to pass on to the next generation of learners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_produce(p_AdjN, p_NumN):\n",
    "    AdjN = binomial(n=40, p=p_AdjN) # number of Adj-N out of n trials with p=p_AdjN\n",
    "    NumN = binomial(n=40, p=p_NumN) # number of Num-N out of n trials with p=p_NumN\n",
    "    return [AdjN, 40-AdjN, NumN, 40-NumN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a pretty horrendous looking function does the iterating. This iterates from starting data consisting of counts of Adj-N, Num-N. It returns number of each pattern type left out of num_samps * 4 in each generation. The steps it follows are: \n",
    "1. get samples from each condition given starting_data\n",
    "2. pick random grammar from each set of samples and use to generate new starting_data for that condition\n",
    "3. count the number of each patterns resulting from those samples\n",
    "4. REPEAT\n",
    "\n",
    "This function returns a list of lists. Each list in there tracks the counts for one of the four patterns over generations. If you ran 3 generations there’d be three numbers in each list representing how many of each pattern resulted from learning in that generation.\n",
    "\n",
    "There are four loops. The outermost loop goes through the generations (as many as the user specifies). The second loop goes through each condition for the current generation, samples some grammars from the posterior for that condition and picks one of them to generate new training data from, to pass on to the next generation. The third loop classifies all the sampled patterns. The fourth loop puts the counts of each pattern for the current generation into the value of the function to be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U18_iterate(starting_data, g, a, b, generations, num_samps):\n",
    "    \n",
    "    pattern_tracer=[[], [], [], []] # value of the function, each sublist \n",
    "                                    # tracks count of each pattern type over generations   \n",
    "    for gen in range(generations):\n",
    "        patterns_g = [] # accumulator for pattern types in each sample for current generation\n",
    "        new_data=[[], [], [], []] # to be used as starting_data in the subsequent generation\n",
    "        \n",
    "        # for each condition, get sample of grammars, generate new training data, \n",
    "        # count patterns...\n",
    "        for i in range(4):\n",
    "            print(\"sampling for condition \" + str(i+1) + \"...\")\n",
    "            samps_i = U18_roulette_wheel(g, a, b, starting_data[i], num_samps) # get sample of grammars\n",
    "            r = random.randint(0, num_samps - 1)# pick a random index from samps\n",
    "            training_g = samps_i[r] # get the grammar at index r \n",
    "            print(\"learner has acquired: \" + str(training_g))\n",
    "            new_data[i] = U18_produce(training_g[0], training_g[1]) # use grammar to generate new data\n",
    "            print(\"passing on:\" + str(new_data[i]) + \" to next generation...\")\n",
    "            # now for each grammar in the sample, classify it and add pattern to the accumulator\n",
    "            for s in range(len(samps_i)):\n",
    "                patterns_g.append(U18_classify(samps_i[s][0], samps_i[s][1]))\n",
    "        \n",
    "        # go through pattern accumulator and count each type for the current generation\n",
    "        for i in range(4): \n",
    "            pattern_tracer[i].append(patterns_g.count(i + 1)) # add the set of patterns to list\n",
    "        print(\"Frequency of patterns for this generation: \" + str(pattern_tracer))\n",
    "            \n",
    "        starting_data = new_data # make the new training data the starting data\n",
    "    \n",
    "    return pattern_tracer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do some iterating, call the function with the following arguments. It will take awhile, and you’ll notice that the function has some print statements so you know what it’s up to while you’re waiting.\n",
    "\n",
    "```python\n",
    "counts = U18_iterate(training_data, fit_parameters[0], fit_parameters[1], fit_parameters[2], 3, 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the outcome of the iterated learning simulation, you can use the following code:\n",
    "\n",
    "```python\n",
    "plt.plot(counts[0], color='red', label='pattern 1')\n",
    "plt.plot(counts[1], color='blue', label='pattern 2')\n",
    "plt.plot(counts[2], color='orange', label='pattern 3')\n",
    "plt.plot(counts[3], color='purple', label='pattern 4')\n",
    "plt.xlabel('generations')\n",
    "plt.ylabel('count')\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    "This will show you which patterns survive, and which die out over generations. Do you think this is a realistic result? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
