{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Language 23, Extending Bayesian Iterated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simulation implements a simplified version of the language model from Kirby, Dowman & Griffiths (2007) using an explicit agent-based simulation, and embeds this language model in a slightly more sophisticated population model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')\n",
    "\n",
    "from math import log, log1p, exp\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Kirby, Dowman & Griffiths (2007), we assume a language is made up of a set of *variables*, each of which can exist in a number of different *variant* forms. This is a rather general characterisation that actually applies well to a number of linguistic phenomena. For example, we can think of the variables as different syntactic categories, and the variants as word orders. Alternatively, the variables could be verb-meanings and the variants different realisations of the past tense, and so on. Agents will produce (and learn from) data which simply exemplifies which variant they have for a particular variable (with the possibility of noise on transmission). We will group languages into two classes: regular languages (where the same variant is used for all variables) and irregular languages (where more than one variant is used).\n",
    "\n",
    "As usual, the new code starts with a set of parameter declarations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = 'sample'     # The type of learning ('map' or 'sample')\n",
    "bias = log(0.6)         # The preference for regular languages\n",
    "variables = 2           # The number of different variables in the language\n",
    "variants = 2            # The number of different variants each variable can take\n",
    "noise = log(0.05)       # The probability of producing the wrong variant\n",
    "population_size = 1000  # Size of population\n",
    "teachers = 'single'     # Either 'single' or 'multiple'\n",
    "method = 'chain'        # Either 'chain' or 'replacement'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for dealing with log probabilities\n",
    "\n",
    "Here are our standard functions for dealing with logs, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_subtract(x,y):\n",
    "    return x + log1p(-exp(y - x))\n",
    "\n",
    "def normalize_logprobs(logprobs):\n",
    "    logtotal = logsumexp(logprobs) #calculates the summed log probabilities\n",
    "    normedlogs = []\n",
    "    for logp in logprobs:\n",
    "        normedlogs.append(logp - logtotal) #normalise - subtracting in the log domain\n",
    "                                        #equivalent to dividing in the normal domain\n",
    "    return normedlogs\n",
    " \n",
    "def log_roulette_wheel(normedlogs):\n",
    "    r = log(random.random()) #generate a random number in [0,1), then convert to log\n",
    "    accumulator = normedlogs[0]\n",
    "    for i in range(len(normedlogs)):\n",
    "        if r < accumulator:\n",
    "            return i\n",
    "        accumulator = logsumexp([accumulator, normedlogs[i + 1]])\n",
    "\n",
    "def wta(items):\n",
    "    maxweight = max(items)\n",
    "    candidates = []\n",
    "    for i in range(len(items)):\n",
    "        if items[i] == maxweight:\n",
    "            candidates.append(i)\n",
    "    return random.choice(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce(language):\n",
    "    variable = random.randrange(len(language))\n",
    "    correct_variant = language[variable]\n",
    "    if log(random.random()) > noise:\n",
    "        return [variable, correct_variant]\n",
    "    else:\n",
    "        possible_noise_variants = list(range(variants))\n",
    "        possible_noise_variants.remove(correct_variant)\n",
    "        noisy_variant = random.choice(possible_noise_variants)\n",
    "        return [variable, noisy_variant]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function produce takes a language, selects a random variable, and produces the relevant variant from the language.\n",
    "\n",
    "- By looking at this code, can you tell how languages are represented in the simulation?\n",
    "- Can you see how ‘noise’ - errors on production - works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying languages\n",
    "\n",
    "In this language model, prior probability is determined by language class: regular languages differ from irregular languages in their prior probability, and ultimately we are interested in the proportion of our simulated population who use regular languages. We therefore need a function to take a language and classify it as regular or not - the function `regular` does this. We also want to be able to calculate the proportion of individuals in a population who use a regular language (for plotting simulation results), which is done by `proportion_regular_language`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular(language):\n",
    "    regular = True\n",
    "    first_variant = language[0]\n",
    "    for variant in language:\n",
    "        if variant != first_variant:\n",
    "            regular = False\n",
    "    return regular\n",
    "\n",
    "def proportion_regular_language(population):\n",
    "    regular_count = 0\n",
    "    for agent in population:\n",
    "        if regular(agent):\n",
    "            regular_count += 1\n",
    "    return regular_count / float(len(population))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprior(language):\n",
    "    if regular(language):\n",
    "        number_of_regular_languages = variants\n",
    "        return bias - log(number_of_regular_languages) #subtracting logs = dividing\n",
    "    else:\n",
    "        number_of_irregular_languages = pow(variants, variables) - variants\n",
    "        return log_subtract(0, bias) - log(number_of_irregular_languages)\n",
    "        # log(1) is 0, so log_subtract(0, bias) is equivalent to (1 - bias) in the\n",
    "        # non-log domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `logprior` returns the prior probability (as a log probability) of a particular language. The strength of preference for regular languages depends on the simulation parameter `bias` - if bias is over 0.5 (when converted back from a log probability), regular languages have higher prior probability.\n",
    "\n",
    "- Why are we dividing the bias by the number of regular and irregular languages in this function? Check you understand how these numbers are calculated.\n",
    "- How does this function differ from the prior from the Kirby, Dowman & Griffiths (2007) paper? (Hint: consider the case of more than two variables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(data, language):\n",
    "    loglikelihoods = []\n",
    "    logp_correct = log_subtract(0, noise) #probability of producing correct form\n",
    "    logp_incorrect = noise - log(variants - 1) #logprob of each incorrect variant\n",
    "    for utterance in data:\n",
    "        variable = utterance[0]\n",
    "        variant = utterance[1]\n",
    "        if variant == language[variable]:\n",
    "            loglikelihoods.append(logp_correct)\n",
    "        else:\n",
    "            loglikelihoods.append(logp_incorrect)\n",
    "    return sum(loglikelihoods) #summing log likelihoods = multiplying likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `loglikelihood` takes a language and a list of data and works out the (log) likelihood of the data given the language. We allows some small probability (given by the simulation parameter `noise`) that a speaker will produce the ‘wrong’ variant, i.e. a variant other than that specified by their language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Bayesian learners calculate the posterior probability of each language based on some data, then select a language (‘learn’) based on those posterior probabilities. `learn` implements this. As discussed in the lecture, there are two ways you could select a language based on the posterior probability distribution:\n",
    "- You could pick the best language - i.e. the language with the highest posterior probability. This is called MAP (“maximum a posteriori”) learning.\n",
    "- Alternatively, you could pick a language probabilistically based on its posterior probability, without necessarily going for the best one every time (e.g. if language 0 has twice the posterior probability of language 1, you are twice as likely to pick it). This is called sampling (for “sampling from the posterior distribution”).\n",
    "\n",
    "The next bit of code implements both these ways of learning, using the familiar `wta` function to do MAP learning and using `log_roulette_wheel` to do sampling (from previous labs, which assumed learners sample from the posterior). `all_languages` enumerates all possible languages for expressing `n` variables using a cute recursive method (don’t worry too much if you can’t figure out how it works, but you might get an idea if you figure out what steps it would take when called with different arguments, like `all_languages(0)`, `all_languages(1)` and so on). Finally, `learn` implements hypothesis selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_languages(n):\n",
    "    if n == 0:\n",
    "        return [[]]\n",
    "    else:\n",
    "        result = []\n",
    "        smaller_langs = all_languages(n - 1)\n",
    "        for l in smaller_langs:\n",
    "            for v in range(variants):\n",
    "                result.append(l + [v])\n",
    "        return result\n",
    "\n",
    "def learn(data):\n",
    "    list_of_all_languages = all_languages(variables)\n",
    "    list_of_posteriors = []\n",
    "    for language in list_of_all_languages:\n",
    "        this_language_posterior = loglikelihood(data, language) + logprior(language)\n",
    "        list_of_posteriors.append(this_language_posterior)\n",
    "    if learning == 'map':\n",
    "        map_language_index = wta(list_of_posteriors)\n",
    "        map_language = list_of_all_languages[map_language_index]\n",
    "        return map_language\n",
    "    if learning == 'sample':\n",
    "        normalized_posteriors = normalize_logprobs(list_of_posteriors)\n",
    "        sampled_language_index = log_roulette_wheel(normalized_posteriors)\n",
    "        sampled_language = list_of_all_languages[sampled_language_index]\n",
    "        return sampled_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simulation\n",
    "\n",
    "There are two main functions to actually carry out the relevant simulation runs. The first is `pop_learn`, creates a new population of a specified size who learn a language from data produced by an adult population. It calls on the `teachers` global parameter to decide whether these learners should learn from a single individual in the adult population, or whether they learn each utterance from a randomly-selected member of the adult population (i.e. learns from multiple teachers).\n",
    "- How is the difference between single and multiple teachers implemented? In the multiple-teacher version, is each data item guaranteed to be produced by a separate teacher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_learn(adult_population,bottleneck,number_of_learners):\n",
    "    new_population = []\n",
    "    for n in range(number_of_learners):\n",
    "        if teachers == 'single':\n",
    "            potential_teachers = [random.choice(adult_population)]\n",
    "        if teachers == 'multiple':\n",
    "            potential_teachers = adult_population\n",
    "        data = []\n",
    "        for n in range(bottleneck):\n",
    "            teacher = random.choice(potential_teachers)\n",
    "            utterance = produce(teacher)\n",
    "            data.append(utterance)\n",
    "        learner_grammar = learn(data)\n",
    "        new_population.append(learner_grammar)\n",
    "    return new_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`initial_population` is a subsidiary function which generates a population of a specified size of individuals speaking randomly-selected languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_population(n):\n",
    "    population = []\n",
    "    possible_languages = all_languages(variables)\n",
    "    for agent in range(n):\n",
    "        language=random.choice(possible_languages)\n",
    "        population.append(language)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second main function is `iterate`, which is the top-level function which actually runs simulations. This function calls on the method `parameter`, to run either chain simulations (where a population consists of a series of generations, where the entire population is replaced at each generation) or replacement simulation (where a single individual is replaced at each ‘generation’). It returns a list of two things: the final population, and a (plottable) list of the proportion of each generation which uses a regular language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(generations, bottleneck, report_every):\n",
    "    population = initial_population(population_size)\n",
    "    accumulator=[proportion_regular_language(population)]\n",
    "    for g in range(1, generations + 1):\n",
    "        if method == 'chain': # Replace whole population\n",
    "            population = pop_learn(population, bottleneck, population_size)\n",
    "        if method == 'replacement': #Replace one individual at a time\n",
    "            population = population[1:]\n",
    "            new_agent = pop_learn(population, bottleneck, 1)[0]\n",
    "            population.append(new_agent)\n",
    "        if (g % report_every == 0):\n",
    "            print(g, end=\" \") # This line is just to let you know the simulation is working.\n",
    "                              # Delete it if it gets annoying! The \"end\" bit tells python not\n",
    "                              # to add a new line each time.\n",
    "            accumulator.append(proportion_regular_language(population))\n",
    "    return population, accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Note:** Running the simulations takes a little time, particularly if you run large populations for large numbers of generations. In general, you probably want to keep the bottleneck values between 1 and 10, in which case you should get representative results within 100 to 500 generations (for chain populations). Larger populations (e.g. 1000 individuals) generally give you cleaner results (have a think about why this is).\n",
    "\n",
    "1. Using the default parameters (single teacher, chain method), check that you can replicate the standard results for sampling and MAP learners: convergence to the prior for samplers, exaggeration of the prior for MAP.\n",
    "\n",
    "2. What happens if you switch from single teachers to multiple teachers? Does the sampler result change? Does the MAP result change? How does the bottleneck effect these results?\n",
    "\n",
    "3. Finally, what happens if you switch from the chain method to the replacement method? Don’t forget that each ‘generation’ in a replacement simulation just replaces a single individual, so you’ll have to run the simulations for lots more generations to get equivalent results to those you got under the chain method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
