{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Language 10, From evolution to learning (lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a first draft of lecture notes for the Simulating Language course. It probably contains lots of typos!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary so far\n",
    "\n",
    "So far in this course we've looked at the evolution of innate optimal signalling. We treated a communication system as a pair of matrices:\n",
    "\n",
    "- A production matrix, where rows are meanings and columns are signals\n",
    "- A reception matrix, where rows are signals and columns are meanings\n",
    "\n",
    "Values in these matrices represent in some way the strength of association between meanings and signals, such that higher numbers are somehow more strongly associated. The precise way this maps onto behaviour depends on the algorithm used to \"interpret\" this matrix. We've been using a \"winner take all\" algorithm which assumes that all that matters is the strongest association on a row.\n",
    "\n",
    "In our model, these numbers in the matrices correspond to what is innate about communication for an agent. In other words, we assume that the numbers arise straightforwardly from a set of genes that the agent inherits. Evolution by natural selection can lead to *adaptation* of these genes. Our research question then was to discover the conditions under which natural selection will lead to genes that give rise to optimal signalling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick simplification\n",
    "\n",
    "Before we move on to the next topic, we are going to simplify our representation slightly. Instead of having two matrices (one for production and one for reception) we're going to replace this with a single matrix for both production and reception:\n",
    "\n",
    "|.    |$s_1$|$s_2$|$s_3$|\n",
    "|-----|-----|-----|-----|\n",
    "|$m_1$|1    |2    |0    |\n",
    "|$m_2$|0    |1    |1    |\n",
    "|$m_3$|0    |3    |4    |\n",
    "\n",
    "For production, this works the same way as before: we read along the rows to find the signal to produce. For reception on the other hand, we read down the columns.\n",
    "\n",
    "On the face of it, it might appear that collapsing the two matrices to one would guarantee that an agent will be self consistent - that their production and reception behaviour would match - given that the same set of numbers is being used for both. However, a quick inspection of the example matrix above demonstrates that this is not necessarily the case. For example, $m_1$ is produced as $s_2$ but $s_2$ is understood as $m_3$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "We have not said much about what these numbers in these matrices actually correspond to in reality. In the case of the vervet monkey they presumably relate in some way to how the brain of the vervet is wired up. Our model therefore assumes that there is some way in which the genes of the vervet lead directly to the brain wiring of the adult monkey. In other words, the genotype of the monkey is the sole determinant of the phenotype (specifically in this case, the properties of the monkey's brain that give rise to its signalling behaviour).\n",
    "\n",
    "*Connectionism* is an approach in cognitive science that foregrounds the relationship between models and the nature of the brain, or at least takes inspiration from the way brains work in building its models (see [here](https://plato.stanford.edu/entries/connectionism/) for an introduction to the philosophy of connectionism). The principal tool of connectionism is the *artificial neural network*, a (very) abstract model loosely based on how the brain works. The idea is that the brain is made up of simple computational units - neurons - that sum up \"activation\" coming from their inputs and use them to decide whether or not to produce an output.\n",
    "\n",
    "![img](img/Neuron.png)\n",
    "\n",
    "Typically, in an artificial neural network, models of these simple neurons are nodes arranged in a network (similarly to a real brain), so the activation from one neuron spreads to others, which then process this activation and spread it further in the network. Mnay artificial neural network models have nodes arranged in layers, with some layers interfacing with \"input\" and/or \"output\". In this way, they can process stimuli to produce behaviour, for example.\n",
    "\n",
    "![img](img/Network.png)\n",
    "\n",
    "Cruciall, connections between neurons are *weighted*. In other words, they modify the signals passing along them. We can think of these patterns of weights as representing the knowledge encoded by the network.\n",
    "\n",
    "### We've been building networks all along\n",
    "\n",
    "It may not be immediately obvious, but our matrix-based model of signalling can be thought of as a neural network. A matrix of associations between meanings and signals can be translated directly into a two layer weighted neural network. The following three graphical representations are after all perfectly equivalent:\n",
    "\n",
    "![img](img/Matrix.png)\n",
    "![img](img/Associative_net1.png)\n",
    "![img](img/Associative_net2.png)\n",
    "\n",
    "Looked at this way, the matrix represents a two layer bi-directional network in which either layer can be input or output. In production, meaning nodes are activated, and activation flows through connections to the output nodes (and vice-versa). The cells in the matrix correspond to weights on the connections between nodes.\n",
    "\n",
    "Activation down a connection is multiplied by the weight on that connection. So, for example, if the $m_1$ node is given an activation of 1 then the activation fed into the $s_1$ node is 1 multiplied by the connection weight, 1. To work out the total activation at an output node, we simply add up all the weighted activations leading into that node.\n",
    "\n",
    "If we represent meanings (or signals) as a pattern of activation over input (or output) nodes, then this neural network behaves identically to our matrix model. For example, imagine we want to produce a signal for meaning 2. The first input neuron is given activation 0, the second activation 1. The activation flowing into $s_1$ is therefore: $(0\\times 1)+(0\\times 0)=0$. The activation flowing into $s_2$ is: $(1\\times 0)+(1\\times 1)=1$. The winner take all algorithm can be applied to output activations as well, and the actual signal produced chosen among the ones with the greatest activation.\n",
    "\n",
    "The point I'm making here is that our matrix model of signalling behaves just as if it were a simple neural network model all along. In fact, whether we choose to call it a matrix of signal associations, or an *associative neural network* is just that - a choice we're making as modellers. Just calling it one thing or the other changes nothing. You might hear this referred to as *formal equivalence* in modelling papers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "So far, we've assumed that agents are born with their knowledge set in their connection weights, and we've allowed biological evolution to \"decide\" what the weights should be. This assumption arises from our decision to equate genotype and phenotype. In reality genes are never the sole determinant of an organism's phenotype. Many aspects of how an organism turns out are reliant on features of the environment. The process of development can be thought of as a function that takes information from both the genes and the environment and gives an organism as output.\n",
    "\n",
    "When it comes to brains the impact of the environment in development is particularly important. We refer to this impact as *learning*. The state of the brain of an individual is to a large part shaped by its experience (and its genes too, but we'll return to this later in the course).\n",
    "\n",
    "We can model learning in our associative network very easily. We're going to assume that at least some of the time, agents are able to observe both meanings and signals occuring simultaneously in the environment. This is akin to a situation where the environment makes it very clear what meaning is being referred to when a signal is produced, for example because it is highly salient in the situation, or because the learner is able to infer what the speaker intends by other means. In these situations then, nodes on both the meaning layer and the signal layer will be activated at the same time. This is quite a big assumption of course, but there are ways of relaxing this assumption by adding uncertainty to the inference of meanings (see e.g., [Smith, Blythe & Smith, 2011](http://www.lel.ed.ac.uk/~kenny/publications/smith_11_crosssituational.pdf)).\n",
    "\n",
    "One of the earliest theories of learning in neural networks was due to [Hebb (1949)](http://s-f-walker.org.uk/pubsebooks/pdfs/The_Organization_of_Behavior-Donald_O._Hebb.pdf): \"any two cells or systems of cells that are repeatedly active at the same time will tend to become 'associated', so that activity in one facilitates activity in the other.\"\n",
    "\n",
    "A straightforward approach to adding Hebbian learning to our network is to start with weights set to zero initially, and then increase connection weights whenever two nodes fire together.\n",
    "\n",
    "Let's work through an example with two meanings and two signals.\n",
    "\n",
    "We start with a zero-ed network/matrix:\n",
    "\n",
    "|.    |$s_1$|$s_2$|\n",
    "|-----|-----|-----|\n",
    "|$m_1$|0    |0    |\n",
    "|$m_2$|0    |0    |\n",
    "\n",
    "Then we see $m_1$ and $s_1$ together, so the connection weight between the two is increased:\n",
    "\n",
    "|.    |$s_1$|$s_2$|\n",
    "|-----|-----|-----|\n",
    "|$m_1$|1    |0    |\n",
    "|$m_2$|0    |0    |\n",
    "\n",
    "And then $m_2$ and $s_2$:\n",
    "\n",
    "|.    |$s_1$|$s_2$|\n",
    "|-----|-----|-----|\n",
    "|$m_1$|1    |0    |\n",
    "|$m_2$|0    |1    |\n",
    "\n",
    "\n",
    "And then $m_1$ and $s_1$ again:\n",
    "\n",
    "|.    |$s_1$|$s_2$|\n",
    "|-----|-----|-----|\n",
    "|$m_1$|2    |0    |\n",
    "|$m_2$|0    |1    |\n",
    "\n",
    "and so on...\n",
    "\n",
    "This kind of Hebbian-like learning procedure is formally equivalent to keeping a frequency count of pairings between meanings and signals seen. It's the simplest model of learning I can think of for our matrix of signals and meanings.\n",
    "\n",
    "It turns out we can modify our existing code for innate signalling with just two extra lines of python. (Although we need some extra stuff to deal with the change from two matrices for production and reception down to just a single one.)\n",
    "\n",
    "The key research questions we might want to ask at this point are:\n",
    "\n",
    "- is this model a sufficient one for modelling the learning of signalling behaviour?\n",
    "- what can an agent with this learning algorithm actually acquire?\n",
    "- and does modelling learning give another route to explaining where optimal signalling comes from?\n",
    "\n",
    "The last point is critical. We know that the meaning-signal mappings we use in human languages are not innately encoded, but rather arise from a process of learning. This means we can't necessarily use the same argumentation for explaining the origins of optimal signalling in language as we were able to do for innate communication systems in the rest of nature. The puzzle of why languages are useful for communication is still very much open. It is to this question we apply our model next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
